 
 	 1.The formula to estimate the number of data nodes (n): 	n= H/d = c*r*S/(1-i)*d 
 	c = average compression ratio. It depends on the type of compression used (Snappy, LZOP, ...) and size of the data. 
 	When no compression is used, c=1. 
 	r = replication factor. It is usually 3 in a production cluster. 
 	S = size of data to be moved to Hadoop. This could be a combination of historical data and
 	incremental data. The incremental data can be daily for example and projected over a period of time (3 years for example). 
 	i = intermediate factor. It is usually 1/3 or 1/4. Hadoop's working space dedicated to storing intermediate results of Map phases. 
 	d= disk space available per node. All other parameters remain the same as in 1. 
 	 
 	here, H=600TB
 	d=7TB
 	therefore, n=H/d
 	n=600/7
 	=85.71
 	total number of data nodes=86.
 	 
 
 	 
 	2. Although the default blocks size is 64 MB in Hadoop 1x and 128 MB in Hadoop 2x whereas in such a scenario let
 	us consider block size to be 100 MB which means that we are going to have 5 blocks replicated 3 times (default replication factor).
 	Let, We have 5 blocks (A/B/C/D/E) for a file, a client, a namenode and a datanode. So, first the client will take Block A and
 	will approach namenode for datanode location to store this block and the replicated copies. Once client is aware about the datanode 
 	information, it will directly reach out to datanode and start copying Block A which will be simultaneously replicated to other 2 datanodes.
 	Once the block is copied and replicated to the datanodes, client will get the confirmation about the Block A storage and then, it
 	will initiate the same process for next block “Block B”.
 	So, during this process if 1st block of 100 MB is written to HDFS and the next block has been started by the client to store then
 	1st block will be visible to readers. Only the current block being written will not be visible by the readers.
